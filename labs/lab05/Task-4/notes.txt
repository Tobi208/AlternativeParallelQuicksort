For memset I expect FAST to be a lot faster than SLOW because it doesn't use loops and memset and memmove are probably highly optimized. I don't expect much difference for with optimize arguments though.

SLOW, no opt: 0,732s
SLOW,    opt: 0,049s
FAST, no opt: 0,041s
FAST,    opt: 0,052s

Looks like memset/memmove is actually the optimization the compiler does.



For lookup I expect FAST to be much, much, much faster than SLOW for opt and no opt executables since a single 'lookup' takes practically no time compared to the SLOW loop even, optimized loops probably.

SLOW, no opt: 1,307s
SLOW,    opt: 0,351s
FAST, no opt: 0,488s
FAST,    opt: 0,091s

The optimized SLOW is actually faster than the unoptimized FAST, but that's probably due to optimizing the loop in the main function.

Pure functional functions with a "worthwhile" number of inputs are good candidates for this optimization. The tradeoff between time and space complexity has to be evaluated here.



Optimizations done for strength reduction: bit shifting, unsigned int division, using constants, and floating point multiplication instead of division

SLOW, no opt: 1,766s
SLOW,    opt: 0,577s
FAST, no opt: 0,926s
FAST,    opt: 0,364s



Measurements for math functions:

v1, no opt: 1,839s
v1,    opt: 0,000s
v2, no opt: 0,391s
v2,    opt: 0,000s
v3, no opt: 0,371s
v3,    opt: 0,000s

x = sqrtf(i); seems to be the fastest. It makes sense that sqrt is more optimized than pow with a < 1 exponent. sqrtf is faster than sqrt because it is for floats instead of doubles and has to deal less with precision.


